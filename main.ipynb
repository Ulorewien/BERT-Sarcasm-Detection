{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import NewsHeadlinesDataset\n",
    "from model import SarcasmDetectionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "news_dataset_dir = \"Datasets\\\\News_Headlines\\\\Sarcasm_Headlines_Dataset_v2.json\"\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Hyperparameters\n",
    "len_dataset = 2000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Instances of each sample in the entire dataset: is_sarcastic\n",
      "0    1025\n",
      "1     975\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "news_dataset = pd.read_json(news_dataset_dir, lines=True)\n",
    "news_dataset = news_dataset[:len_dataset]\n",
    "print(f\"Instances of each sample in the entire dataset: {news_dataset[\"is_sarcastic\"].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, \"bert-base-uncased\")\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "bert_model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the padded dataset: (2000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_dataset_news = news_dataset[\"headline\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# Padding to match the shapes of all the input data\n",
    "max_len = 0\n",
    "for i in tokenized_dataset_news.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_dataset_news = np.array([i + [0]*(max_len-len(i)) for i in tokenized_dataset_news.values])\n",
    "print(f\"Shape of the padded dataset: {np.array(padded_dataset_news).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the masked dataset: (2000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Add a mask so that the model doesn't consider padded tokens\n",
    "masked_dataset_news = np.where(padded_dataset_news != 0, 1, 0)\n",
    "print(f\"Shape of the masked dataset: {masked_dataset_news.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the data -> Train (1600) & Test (400)\n"
     ]
    }
   ],
   "source": [
    "# Split the data in training and testing sets and create dataloaders\n",
    "split_val = int(split_ratio*len_dataset)\n",
    "print(f\"Splitting the data -> Train ({split_val}) & Test ({len_dataset-split_val})\")\n",
    "train_features_news = masked_dataset_news[:split_val]\n",
    "test_features_news = masked_dataset_news[split_val:]\n",
    "train_labels_news = news_dataset[\"is_sarcastic\"].values[:split_val]\n",
    "test_labels_news = news_dataset[\"is_sarcastic\"].values[split_val:]\n",
    "\n",
    "train_dataset_news = NewsHeadlinesDataset(train_features_news, train_labels_news)\n",
    "test_dataset_news = NewsHeadlinesDataset(test_features_news, test_labels_news)\n",
    "train_loader_news = DataLoader(train_dataset_news, batch_size=batch_size, shuffle=True)\n",
    "test_loader_news = DataLoader(test_dataset_news, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
