{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import NewsHeadlinesDataset\n",
    "from model import SarcasmDetectionModel\n",
    "from util import train_model, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "news_dataset_dir = \"Datasets\\\\News_Headlines\\\\Sarcasm_Headlines_Dataset_v2.json\"\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Hyperparameters\n",
    "len_dataset = 2000\n",
    "batch_size = 32\n",
    "lr = 2e-5\n",
    "n_epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Instances of each sample in the entire dataset: is_sarcastic\n",
      "0    1025\n",
      "1     975\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "news_dataset = pd.read_json(news_dataset_dir, lines=True)\n",
    "news_dataset = news_dataset[:len_dataset]\n",
    "print(f\"Instances of each sample in the entire dataset: {news_dataset[\"is_sarcastic\"].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, \"bert-base-uncased\")\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "bert_model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the padded dataset: (2000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_dataset_news = news_dataset[\"headline\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# Padding to match the shapes of all the input data\n",
    "max_len = 0\n",
    "for i in tokenized_dataset_news.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_dataset_news = np.array([i + [0]*(max_len-len(i)) for i in tokenized_dataset_news.values])\n",
    "print(f\"Shape of the padded dataset: {np.array(padded_dataset_news).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the masked dataset: (2000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Add a mask so that the model doesn't consider padded tokens\n",
    "attention_mask_news = np.where(padded_dataset_news != 0, 1, 0)\n",
    "print(f\"Shape of the masked dataset: {attention_mask_news.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the data -> Train (1600) & Test (400)\n"
     ]
    }
   ],
   "source": [
    "# Split the data in training and testing sets and create dataloaders\n",
    "split_val = int(split_ratio*len_dataset)\n",
    "print(f\"Splitting the data -> Train ({split_val}) & Test ({len_dataset-split_val})\")\n",
    "\n",
    "train_features_news = torch.Tensor(padded_dataset_news[:split_val], device=device).long()\n",
    "train_mask_news = torch.Tensor(attention_mask_news[:split_val], device=device).long()\n",
    "train_labels_news = torch.Tensor(news_dataset[\"is_sarcastic\"].values[:split_val], device=device).long()\n",
    "train_dataset_news = NewsHeadlinesDataset(train_features_news, train_mask_news, train_labels_news)\n",
    "train_loader_news = DataLoader(train_dataset_news, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_features_news = torch.Tensor(padded_dataset_news[split_val:], device=device).long()\n",
    "test_mask_news = torch.Tensor(attention_mask_news[split_val:], device=device).long()\n",
    "test_labels_news = torch.Tensor(news_dataset[\"is_sarcastic\"].values[split_val:], device=device).long()\n",
    "test_dataset_news = NewsHeadlinesDataset(test_features_news, test_mask_news, test_labels_news)\n",
    "test_loader_news = DataLoader(test_dataset_news, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, optimizer and the loss function\n",
    "model = SarcasmDetectionModel(bert_model).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate the model\n",
    "for epoch in range(n_epoch):\n",
    "    train_model(model, optimizer, train_loader_news, loss_function)\n",
    "    evaluate_model(model, test_loader_news, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"first_run.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
