{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import NewsHeadlinesDataset\n",
    "from model import SarcasmDetectionModel\n",
    "from util import train_model, plot_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "news_dataset_dir = \"Datasets\\\\News_Headlines\\\\Sarcasm_Headlines_Dataset_v2.json\"\n",
    "split_ratio = 0.8\n",
    "model_save_path = \"second_run.pth\"\n",
    "loss_save_path = \"losses.png\"\n",
    "acc_save_path = \"accuracies.png\"\n",
    "\n",
    "# Hyperparameters\n",
    "len_dataset = 2000\n",
    "batch_size = 32\n",
    "lr = 2e-5\n",
    "n_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Instances of each sample in the entire dataset: is_sarcastic\n",
      "0    1025\n",
      "1     975\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "news_dataset = pd.read_json(news_dataset_dir, lines=True)\n",
    "news_dataset = news_dataset[:len_dataset]\n",
    "print(f\"Instances of each sample in the entire dataset: {news_dataset[\"is_sarcastic\"].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, \"bert-base-uncased\")\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "bert_model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the padded dataset: (2000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_dataset_news = news_dataset[\"headline\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# Padding to match the shapes of all the input data\n",
    "max_len = 0\n",
    "for i in tokenized_dataset_news.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_dataset_news = np.array([i + [0]*(max_len-len(i)) for i in tokenized_dataset_news.values])\n",
    "print(f\"Shape of the padded dataset: {np.array(padded_dataset_news).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the masked dataset: (2000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Add a mask so that the model doesn't consider padded tokens\n",
    "attention_mask_news = np.where(padded_dataset_news != 0, 1, 0)\n",
    "print(f\"Shape of the masked dataset: {attention_mask_news.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the data -> Train (1600) & Test (400)\n"
     ]
    }
   ],
   "source": [
    "# Split the data in training and testing sets and create dataloaders\n",
    "split_val = int(split_ratio*len_dataset)\n",
    "print(f\"Splitting the data -> Train ({split_val}) & Test ({len_dataset-split_val})\")\n",
    "\n",
    "train_features_news = torch.Tensor(padded_dataset_news[:split_val], device=device).long()\n",
    "train_mask_news = torch.Tensor(attention_mask_news[:split_val], device=device).long()\n",
    "train_labels_news = torch.Tensor(news_dataset[\"is_sarcastic\"].values[:split_val], device=device).long()\n",
    "train_dataset_news = NewsHeadlinesDataset(train_features_news, train_mask_news, train_labels_news)\n",
    "train_loader_news = DataLoader(train_dataset_news, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_features_news = torch.Tensor(padded_dataset_news[split_val:], device=device).long()\n",
    "test_mask_news = torch.Tensor(attention_mask_news[split_val:], device=device).long()\n",
    "test_labels_news = torch.Tensor(news_dataset[\"is_sarcastic\"].values[split_val:], device=device).long()\n",
    "test_dataset_news = NewsHeadlinesDataset(test_features_news, test_mask_news, test_labels_news)\n",
    "test_loader_news = DataLoader(test_dataset_news, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, optimizer and the loss function\n",
    "model = SarcasmDetectionModel(bert_model).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays to document progress\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.48583127856254577\n",
      "Test loss: 0.3193091211410669 Test acc: 2630.7692307692305%\n",
      "Training loss: 0.21664601005613804\n",
      "Test loss: 0.281693938259895 Test acc: 2684.6153846153848%\n",
      "Training loss: 0.07154383532702922\n",
      "Test loss: 0.4037904884093083 Test acc: 2638.461538461538%\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the model\n",
    "for epoch in range(n_epoch):\n",
    "    train_loss, train_acc, test_loss, test_acc = train_model(model, optimizer, loss_function, train_loader_news, len(train_labels_news), test_loader_news, len(test_labels_news))\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_progress(train_losses, test_losses, \"Loss Plot\", \"Train Loss\", \"Test Loss\", \"Loss\", loss_save_path)\n",
    "plot_progress(train_accuracies, test_accuracies, \"Accuracy Plot\", \"Train Acc\", \"Test Acc\", \"Accuracy\", acc_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
